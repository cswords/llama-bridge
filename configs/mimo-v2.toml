# Llama-Bridge Configuration for Claude Code
# 
# This configuration file optimizes Llama-Bridge for use with Claude Code CLI.
# It creates separate KV caches for main conversations and background tasks,
# preventing cache pollution between different request types.

# ============================================================
# Model Definition
# Currently only one model is supported, but format allows future expansion
# ============================================================
[models.mimo]
n_ubatch = 512
n_batch = 2048
cache_type_v = 'f16'
cache_type_k = 'f16'
flash_attn = true
n_threads = 4
path = "unsloth/MiMo-V2-Flash-GGUF/UD-Q6_K_XL/"

# ============================================================
# Cache Definitions
# Each cache = independent llama_context with its own KV cache
# ============================================================

[caches.main]
model = "mimo"
n_ctx = 131072                     # 128K for main conversation
description = "Main conversation (Sonnet-level)"

[caches.fast]
model = "mimo"
n_ctx = 16384                      # 16K for background tasks
description = "Background tasks (Haiku-level)"

# ============================================================
# Routing Rules
# Priority: endpoint > model (exact) > match (wildcard)
# First matching rule wins
# ============================================================

# Token counting uses minimal resources
[[routes]]
endpoint = "/v1/messages/count_tokens"
cache = "fast"

# Claude Code's Haiku model requests → fast cache
[[routes]]
match = "*haiku*"
cache = "fast"

# Claude Code's small/fast model requests → fast cache  
[[routes]]
match = "*small*"
cache = "fast"

# All other requests → main cache
[[routes]]
match = "*"
cache = "main"
