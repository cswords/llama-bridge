# Copyright (c) 2026 Llama-Bridge Authors.
# This software is released under the GNU General Public License v3.0.
# See the LICENSE file in the project root for full license information.

# Llama-Bridge Configuration for MiniMax-M2.1 (Q8_0)
# 
# MiniMax-M2.1 is a powerful MoE model with extremely long context support.
# The model natively supports up to 1M+ tokens context, but is capped here
# for practical memory management on consumer hardware.

# ============================================================
# Model Definition
# ============================================================
[models.minimax]
n_ubatch = 512
n_batch = 512
cache_type_v = 'f16'
cache_type_k = 'f16'
flash_attn = true
n_threads = 8
path = "unsloth/MiniMax-M2.1-GGUF/Q8_0/"

# ============================================================
# Cache Definitions
# ============================================================

[caches.main]
model = "minimax"
n_ctx = 131072                     # 128K Context for main tasks
description = "Main conversation"

[caches.fast]
model = "minimax"
n_ctx = 131072                      # 128K Context for background/fast tasks (same as main, isolated)
description = "Background tasks"

# ============================================================
# Routing Rules
# ============================================================

[[routes]]
endpoint = "/v1/messages/count_tokens"
cache = "fast"

[[routes]]
match = "*haiku*"
cache = "fast"

[[routes]]
match = "*small*"
cache = "fast"

[[routes]]
match = "*"
cache = "main"
