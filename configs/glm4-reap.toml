# Llama-Bridge Configuration for GLM-4.7-218B (REAP)
# 
# Extremely large model configuration. Ensure you have sufficient RAM (128GB+).

# ============================================================
# Model Definition
# ============================================================
[models.glm4]
path = "unsloth/GLM-4.7-REAP-218B-A32B-GGUF/Q2_K/GLM-4.7-REAP-218B-A32B-Q2_K-00001-of-00002.gguf"

# ============================================================
# Cache Definitions
# ============================================================

[caches.main]
model = "glm4"
n_ctx = 131072                     # 128K Theoretical Limit. Warning: Requires massive RAM/VRAM for KV cache.
description = "Main conversation"

[caches.fast]
model = "glm4"
n_ctx = 32768
description = "Background tasks"

# ============================================================
# Routing Rules
# ============================================================

[[routes]]
endpoint = "/v1/messages/count_tokens"
cache = "fast"

[[routes]]
match = "*haiku*"
cache = "fast"

[[routes]]
match = "*small*"
cache = "fast"

[[routes]]
match = "*"
cache = "main"
