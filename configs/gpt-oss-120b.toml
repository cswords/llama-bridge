# Llama-Bridge Configuration for gpt-oss-120b
# 

# ============================================================
# Model Definition
# ============================================================
[models.gptoss]
path = "models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf"

# ============================================================
# Cache Definitions
# ============================================================

[caches.main]
model = "gptoss"
n_ctx = 65536                      # 64K Context (Conservative for 120B)
description = "Main conversation"

[caches.fast]
model = "gptoss"
n_ctx = 16384
description = "Background tasks"

# ============================================================
# Routing Rules
# ============================================================

[[routes]]
endpoint = "/v1/messages/count_tokens"
cache = "fast"

[[routes]]
match = "*haiku*"
cache = "fast"

[[routes]]
match = "*small*"
cache = "fast"

[[routes]]
match = "*"
cache = "main"
