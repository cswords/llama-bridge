# Llama-Bridge Technical Whitepaper & Developer Guide

Llama-Bridge æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„æœ¬åœ° LLM æ¡¥æ¥æœåŠ¡ï¼Œæ—¨åœ¨ä¸º GGUF æ¨¡å‹æä¾›åŸç”Ÿçº§çš„ API å…¼å®¹æ€§ä½“éªŒã€‚é€šè¿‡é›†æˆ LiteLLM å’Œ llama.cpp çš„ FFI ç»‘å®šï¼Œå®ƒå…è®¸æœ¬åœ°æ¨¡å‹è¢« Claude Codeã€Cursorã€OpenAI SDK ç­‰ç°ä»£å¼€å‘å·¥å…·ç›´æ¥è°ƒç”¨ï¼Œè€Œæ— éœ€å¤æ‚çš„é…ç½®æˆ–é¢å¤–çš„ HTTP ä¸­è½¬ã€‚

---

## 1. æ ¸å¿ƒæŠ€æœ¯ç‰¹ç‚¹ (Key Features)

### ğŸš€ é›¶ HTTP ä¸­è½¬ (Zero-HTTP Overhead)
Llama-Bridge ä¸ä½¿ç”¨ `llama-server` æˆ–å…¶ä»– HTTP åç«¯ã€‚å®ƒé€šè¿‡ pybind11 å°† llama.cpp ç¼–è¯‘ä¸º Python çš„ C++ æ‰©å±•ï¼Œç›´æ¥åœ¨è¿›ç¨‹å†…è¿›è¡Œ FFI è°ƒç”¨ã€‚
- **ä½å»¶è¿Ÿ**ï¼šæ¶ˆé™¤äº†å†…éƒ¨ç»„ä»¶é—´çš„ HTTP åºåˆ—åŒ–/ååºåˆ—åŒ–å¼€é”€ã€‚
- **çœŸæµå¼**ï¼šToken ç”Ÿæˆåç«‹å³ä¼ é€’ç»™ Python å±‚ï¼Œå®ç°æ¯«ç§’çº§é¦–å­—å“åº”ã€‚

### ğŸ”„ ç»Ÿä¸€é…ç½®ä¸è·¯ç”±ç³»ç»Ÿ (Unified Config & Routing)
æ”¯æŒé€šè¿‡ TOML æ–‡ä»¶å®šä¹‰å¤æ‚çš„è·¯ç”±è§„åˆ™ï¼Œå®ç°å•ä¸€ç«¯å£æ”¯æŒå¤šç§ä¸šåŠ¡åœºæ™¯ã€‚
- **å¤šæ¨¡å‹/å¤šç¼“å­˜**ï¼šæ”¯æŒåŠ è½½ä¸€ä¸ªå¤§æ¨¡å‹ï¼ˆå¦‚ Sonnet çº§ï¼‰ï¼Œå¹¶ä¸ºå…¶é…ç½®å¤šä¸ªéš”ç¦»çš„ KV Cacheã€‚
- **æ™ºèƒ½è·¯ç”±**ï¼šæ ¹æ® API Endpointã€æ¨¡å‹åç§°æˆ–é€šé…ç¬¦ï¼Œè‡ªåŠ¨å°†è¯·æ±‚å¯¼å‘â€œä¸»å¯¹è¯â€ï¼ˆå¤§ä¸Šä¸‹æ–‡ï¼‰æˆ–â€œåå°ä»»åŠ¡â€ï¼ˆå°ä¸Šä¸‹æ–‡ï¼‰ã€‚

### ğŸ§Š å¤š Context ç‰©ç†éš”ç¦» (Multi-Context Isolation)
å½»åº•è§£å†³äº†ä¼ ç»Ÿçš„â€œKV Cache æ±¡æŸ“â€é—®é¢˜ã€‚
- **ç‰©ç†éš”ç¦»**ï¼šæ¯ä¸ª Cache å¯¹åº” C++ å±‚ä¸€ä¸ªç‹¬ç«‹çš„ `llama_context` å®ä¾‹ã€‚
- **çŠ¶æ€ç‹¬ç«‹**ï¼šä¸åŒå®¢æˆ·ç«¯æˆ–ä¸åŒç±»å‹çš„ä»»åŠ¡ï¼ˆå¦‚ä»£ç è¡¥å…¨ vs å¯¹è¯ï¼‰äº’ä¸å¹²æ‰°ã€‚
- **å†…å­˜é«˜æ•ˆ**ï¼šæ‰€æœ‰ Context å…±äº«åŒä¸€ä»½æ¨¡å‹æƒé‡ï¼Œä»… KV Cache å ç”¨ç‹¬ç«‹å†…å­˜ã€‚

### ï¿½ï¸ å¥å£®çš„é”™è¯¯å¤„ç† (Robust Error Handling)
- **Context Overflow Protection**ï¼šå½“è¯·æ±‚è¶…è¿‡æ¨¡å‹ä¸Šä¸‹æ–‡é™åˆ¶æ—¶ï¼Œä¸å†å´©æºƒæˆ–æŒ‚èµ·ï¼Œè€Œæ˜¯èƒ½å¤Ÿæ•è· C++ å¼‚å¸¸å¹¶å°†å…¶è½¬æ¢ä¸ºç¬¦åˆ OpenAI/Anthropic æ ‡å‡†çš„ `400 Bad Request` é”™è¯¯å“åº”ï¼Œå…è®¸å®¢æˆ·ç«¯æ™ºèƒ½å¤„ç†ï¼ˆå¦‚è‡ªåŠ¨æˆªæ–­å†å²ï¼‰ã€‚
- **å‹å¥½æç¤º**ï¼šé”™è¯¯ä¿¡æ¯åŒ…å«è¯¦ç»†çš„ token ç»Ÿè®¡ï¼ˆè¯·æ±‚æ•° vs é™åˆ¶æ•°ï¼‰ï¼Œæ–¹ä¾¿è°ƒè¯•ã€‚

### ğŸ” ç¼“å­˜ä¸€è‡´æ€§ç›‘æµ‹ (Cache Observability)
- **System Prompt Fingerprinting**ï¼šè‡ªåŠ¨æ£€æµ‹ System Prompt çš„å˜æ›´ã€‚å¦‚æœæ£€æµ‹åˆ°åŒä¸€ä¸ª Cache Slot çš„ System Prompt å‘ç”Ÿå“ˆå¸Œå˜åŒ–ï¼Œä¼šåœ¨æ—¥å¿—ä¸­å‘å‡ºè­¦å‘Šï¼Œæç¤ºæ½œåœ¨çš„ Cache Miss æ€§èƒ½å½±å“ã€‚

### ï¿½ğŸ”Œ å…¨åè®®æ”¯æŒ (Universal Protocol Support)
é›†æˆ LiteLLMï¼ˆåº“æ¨¡å¼ï¼‰ï¼Œæ”¯æŒå‡ ä¹æ‰€æœ‰ä¸»æµ LLM API æ ¼å¼ï¼š
- **Anthropic Messages API**: å®Œç¾æ”¯æŒ Claude Codeã€‚
- **OpenAI Chat Completions**: æ”¯æŒ Cursorã€Continue ç­‰å·¥å…·ã€‚
- **å·¥å…·è°ƒç”¨ (Tool Use)**: åˆ©ç”¨ llama.cpp åŸç”Ÿçš„ grammar-constrained generationï¼Œå®ç°é«˜å¯é æ€§çš„å‡½æ•°è°ƒç”¨ã€‚

---

## 2. ç³»ç»Ÿæ¶æ„ (Architecture)

```mermaid
graph TD
    Client[Client (Claude Code / Cursor)] -->|HTTP (Anthropic/OpenAI Protocol)| Server[FastAPI Server]
    
    subgraph "Llama-Bridge Process"
        Server --> Router[Router]
        Router -->|Route Request| Bridge[Bridge Core]
        
        subgraph "Python Logic"
            Bridge -->|Convert Protocol| Adapter[Protocol Adapter (LiteLLM)]
            Bridge -->|Select Context| Wrapper[LlamaChatWrapper (C++)]
        end
        
        subgraph "C++ Binding (pybind11)"
            Wrapper -->|FFI Call| LlamaCPP[libllama.dylib]
            Wrapper -->|Checks| Limits[Overflow Guard]
            
            subgraph "Memory Space"
                LlamaCPP -->|Shared Weights| Model[Model Weights]
                LlamaCPP -->|Context A| Cache1[KV Cache (Main)]
                LlamaCPP -->|Context B| Cache2[KV Cache (Fast)]
            end
        end
    end
```

### æ•°æ®æµå‘
1. **Request**: å®¢æˆ·ç«¯å‘é€ HTTP è¯·æ±‚ï¼ˆå¦‚ `/v1/messages`ï¼‰ã€‚
2. **Routing**: `Router` æ ¹æ®é…ç½®ï¼ˆå¦‚æ¨¡å‹å `claude-3-5-haiku`ï¼‰å†³å®šä½¿ç”¨å“ªä¸ª Cacheï¼ˆå¦‚ `fast`ï¼‰ã€‚
3. **Adaptation**: `Adapter` å°†è¯·æ±‚è½¬æ¢ä¸ºæ ‡å‡†åŒ–çš„å†…éƒ¨æ ¼å¼ã€‚
4. **Validation**: `Bridge` æ£€æŸ¥ System Prompt æŒ‡çº¹ï¼Œå¹¶åœ¨åˆå§‹åŒ–æ¨ç†æ—¶éªŒè¯ Context é™åˆ¶ã€‚
5. **Inference**: `Bridge` è°ƒç”¨ C++ ç»‘å®šï¼ŒæŒ‡å®šç›®æ ‡ Context è¿›è¡Œæ¨ç†ã€‚æ”¯æŒæµå¼ï¼ˆStreamingï¼‰æ™ºèƒ½åˆå¹¶é€»è¾‘ï¼Œå¤„ç†æ€è€ƒå—ï¼ˆThinkingï¼‰çš„æŠ‘åˆ¶æˆ–è¾“å‡ºã€‚
6. **Streaming**: C++ å±‚æ¯ç”Ÿæˆä¸€ä¸ª Tokenï¼Œç«‹å³å›è°ƒ Python å±‚ï¼Œè½¬æ¢ä¸º SSE äº‹ä»¶æ¨é€åˆ°å®¢æˆ·ç«¯ã€‚

---

## 3. å®ç°ç»†èŠ‚ (Implementation Details)

### 3.1 C++ ç»‘å®š (`bindings/llama_chat_wrapper.cpp`)
æˆ‘ä»¬ç»´æŠ¤äº†è‡ªå·±çš„è½»é‡çº§ç»‘å®šï¼Œä»¥ä¾¿ç›´æ¥è®¿é—® `common/chat.h` çš„é«˜çº§åŠŸèƒ½ã€‚
- **Context Management**: å®ç°äº† `create_context(name, n_ctx)` å’Œ `select_context(name)`ã€‚
- **Overflow Checks**: åœ¨ `init_inference` ä¸­ç¡¬æ€§æ£€æŸ¥ `n_prompt + max_tokens > n_ctx`ï¼Œå¹¶åœ¨æº¢å‡ºæ—¶æŠ›å‡ºæ ‡å‡† C++ å¼‚å¸¸ã€‚
- **Chat Templates**: å¤ç”¨äº† llama.cpp å¼ºå¤§çš„ Jinja2 æ¨¡æ¿å¼•æ“ã€‚

### 3.2 Python Bridge (`src/bridge.py`)
è¿æ¥ Web æœåŠ¡å’Œ C++ æ¨ç†çš„æ ¸å¿ƒèƒ¶æ°´å±‚ã€‚
- **é€‚é…å™¨æ¨¡å¼**: `AnthropicAdapter` å’Œ `OpenAIAdapter` å¤„ç†åè®®å·®å¼‚ã€‚
- **æµå¼å¤„ç†**: å®ç°äº†æ™ºèƒ½çš„ `_stream_generate` å¾ªç¯ï¼Œèƒ½å¤Ÿå¤„ç†ç»“æ„åŒ–è¾“å‡ºï¼ˆå¦‚ `<thought>`ï¼‰ï¼Œæ”¯æŒä» C++ è§£æç»“æœæˆ– Python æ­£åˆ™ fallback ä¸­æå–å†…å®¹ã€‚
- **å¼‚å¸¸æ¡¥æ¥**: å°† C++ `RuntimeError` æ˜ å°„ä¸º Python `ContextLimitExceededError`ã€‚

### 3.3 æœåŠ¡å±‚ (`src/server.py`)
- **Global Exception Handlers**: ç»Ÿä¸€æ•è·é€»è¾‘é”™è¯¯å¹¶æ˜ å°„ä¸º API é”™è¯¯æ ¼å¼ï¼ˆå¦‚ `invalid_request_error`ï¼‰ï¼Œç¡®ä¿å®¢æˆ·ç«¯ä¸ä¼šæ”¶åˆ°ä»¤äººå›°æƒ‘çš„ `500 Internal Server Error`ã€‚

---

## 4. æ„å»ºä¸å®‰è£… (Build & Setup)

### å‰ç½®è¦æ±‚
- Python 3.10+
- CMake 3.10+
- C++ ç¼–è¯‘å™¨ (Clang/GCC)
- `uv` (æ¨è) æˆ– `pip`

### æ„å»ºæ­¥éª¤
1. **åˆå§‹åŒ–å­æ¨¡å—** (å¦‚æœæ˜¯ git clone):
   ```bash
   git submodule update --init --recursive
   ```

2. **ç¼–è¯‘ C++ ç»‘å®š**:
   ```bash
   make build
   # è¿™å°†ç”Ÿæˆ llama_chat_wrapper.cpython-*.so å¹¶æ”¾å…¥ src/ ç›®å½•
   ```

3. **å®‰è£… Python ä¾èµ–**:
   ```bash
   uv sync
   ```

---

## 5. ä½¿ç”¨å®ä¾‹ (Usage Guide)

### 5.1 é…ç½®æ–‡ä»¶ (Recommended)

åˆ›å»º `configs/claude-code.toml`ï¼š

```toml
# å®šä¹‰æ¨¡å‹
[models.mimo]
path = "unsloth/MiMo-V2-Flash-GGUF/UD-Q6_K_XL/"

# å®šä¹‰ä¸»ç¼“å­˜ (ç”¨äºå¯¹è¯)
[caches.main]
model = "mimo"
n_ctx = 32768
description = "ä¸»å¯¹è¯ç¼“å­˜"

# å®šä¹‰å¿«é€Ÿç¼“å­˜ (ç”¨äºåå°ä»»åŠ¡/Haiku)
[caches.fast]
model = "mimo"
n_ctx = 8192
description = "åå°ä»»åŠ¡ç¼“å­˜"

# è·¯ç”±è§„åˆ™
[[routes]]
match = "*haiku*"   # æ‰€æœ‰ Haiku æ¨¡å‹è¯·æ±‚å» fast
cache = "fast"

[[routes]]
match = "*"         # å…¶ä»–é»˜è®¤å» main
cache = "main"
```

### 5.2 å¯åŠ¨æœåŠ¡

```bash
# æ¨èï¼šä½¿ç”¨é…ç½®æ–‡ä»¶
uv run serve --config configs/claude-code.toml
```

### 5.3 å®¢æˆ·ç«¯è¿æ¥

**Claude Code**:
```bash
export ANTHROPIC_BASE_URL=http://localhost:8000
claude
```

**Cursor / OpenAI SDK**:
```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="sk-dummy"
)

# å³ä½¿è¯·æ±‚è¿‡é•¿ï¼Œç°åœ¨ä¹Ÿä¼šæ”¶åˆ°æ¸…æ™°çš„ 400 é”™è¯¯
try:
    response = client.chat.completions.create(
        model="claude-sonnet-4",
        messages=[{"role": "user", "content": "..." * 10000}]
    )
except openai.BadRequestError as e:
    print(f"Context Overflow: {e}")
```

---

## 6. å¼€å‘è§„çº¦ (Development Guidelines)

ä¸ºäº†ä¿æŒé¡¹ç›®çš„é«˜è´¨é‡å’Œå¯ç»´æŠ¤æ€§ï¼Œæ‰€æœ‰è´¡çŒ®è€…ï¼ˆåŒ…æ‹¬ AI Agentï¼‰å¿…é¡»éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š

### 6.1 ç›®å½•æƒé™
* **ã€å…è®¸ç¼–è¾‘ã€‘** `src/`, `tests/`, `bindings/`, `configs/`
* **ã€åªè¯»/ç¦åŠ¨ã€‘** `models/` (ä»…é€šè¿‡ hfd.sh ä¸‹è½½), `vendor/` (ä»… git update)
* **ã€ç¦æ­¢è§¦ç¢°ã€‘** `.venv/`

### 6.2 æµ‹è¯•é©±åŠ¨å¼€å‘ (TDD)
* **å˜æ›´æµç¨‹**ï¼šå…ˆå†™æµ‹è¯• -> è¿è¡Œå¤±è´¥ -> ä¿®æ”¹ä»£ç  -> æµ‹è¯•é€šè¿‡ã€‚
* **å›å½’æµ‹è¯•**ï¼šç¡®ä¿æ‰€æœ‰ `tests/unit` å’Œ `tests/integration` ä¸‹çš„æµ‹è¯•ï¼ˆç‰¹åˆ«æ˜¯ `test_overflow_handling.py` å’Œ `test_structured_content.py`ï¼‰æŒç»­é€šè¿‡ã€‚

### 6.3 4-File æ—¥å¿—è§„åˆ™
åœ¨ `--debug` æ¨¡å¼ä¸‹ï¼Œä¸»è¦è¯·æ±‚/å“åº”æ•°æ®ä¼šè¢«è®°å½•ï¼Œç”¨äºè°ƒè¯•åè®®è½¬æ¢é—®é¢˜ã€‚
