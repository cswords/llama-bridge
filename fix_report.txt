Fix Verification Report:
1. Issue: "RuntimeError: Failed to decode prompt" caused by KV cache overflow and mismanagement of ephemeral requests.
2. Fixes Applied:
   - Implemented "Smart Space Check" in `init_inference` to detect KV cache pressure.
   - Added logic to automatically clear Ephemeral sequences (Seq 1) when pressure is detected.
   - Refined `is_ephemeral` detection logic to specifically target requests where context size drops significantly (new < history/2), avoiding accidental cache thrashing.
   - Increased `n_ctx` in `run_optimized.sh` to 8192 to accommodate overhead and prevent "failed to find memory slot" errors during generation.
   - Added robust error handling in `llama_chat_wrapper.cpp` (though logs still show warnings, the application no longer crashes).
3. Verification:
   - `reproduce_issue.py` now runs to completion with "Success" status for all steps.
   - Request 1 (Main Chat): Success.
   - Request 2 (Ephemeral Probe): Success (0.8s).
   - Request 3 (Main Follow-up): Success (15s). 
   - Logs confirm `n_keep` calculation is working (e.g., keeping 571 tokens for Request 3).
4. Remaining Notes:
   - There are still `llama_decode` warnings in the logs regarding sequence position mismatches during the ephemeral request limit boundary, but these do not crash the server or prevent successful response generation.
   - Performance is improved by correctly identifying ephemeral requests and preserving the main sequence cache.
